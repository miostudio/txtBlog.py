机器学习教程【强烈推荐】

小象学院
https://www.bilibili.com/video/BV1Tb411H7uC





========================================
1.机器学习概论(142min)
----------------------------------------
1. 定义: 
机器学习=f(任务T, 经验E, 时间T, 性能P)


问题: 如何设计自动驾驶系统?
边做，边反馈，逐渐修正，越来越正确。

如何从无知，到掌握知识。


有监督的学习: 小孩学月亮
无监督的学习: 阅兵
增强学习: 走路、踢球


加部分标签，其余knn自动加标签：版监督模型。





2. 机器学习的内涵和外延
	数据清晰/特征选择
	确定算法模型/参数优化
	结果预测
#

推导公式、写实现代码，有助于理解调参的意义。
实际工作中，使用现有库，掉包侠。






3. 实例: 线性回归预测房价。
feature: 
	type
	room
	squre
	public transport
#
price:
#
损失函数 Lost(theta)= 累加 (f(t)-realValue)^2
也叫目标函数。

随着theta的变动，损失函数越来越小，则是最优的。


难点: 怎么建模? 目标函数怎么建立?

theta是可以根据模型自动优化的。
还有些超参，只能人工设定，根据经验。


这个预测模型的过程: 
(1)tranining:
Input
	text docs
	images
	sounds
	transactions
i)Feature vectors
ii)Machine learning algorithm
iii)Model


(2)New input
	text docs
	images
	sounds
	transactions
i)Feature vectors
ii)use model;
iii) Expected Label



4. 机器学习的一般流程
数据收集
数据清洗
特征工程
数据建模



5. 模型选择的依据？
先大致了解各种模型的方法
最好会推导，然后具体问题具体分析。


因马尔科夫模型HMM，用于发现新词: 





========================================
|-- 机器学习实例、落地
----------------------------------------

6. 落地语言: Python 
分析
画图: 

# 实例1： 在log曲线上连接两点
import math
import matplotlib.pyplot as plt
def myDraw():
    x=[float(i)/100.0 for i in range(1,300)]
    y=[math.log(i) for i in x]
    plt.plot(x,y, 'r-', linewidth=3, label='log Curve')
    #
    a=[x[20], x[175]]
    b=[y[20], y[175]]
    plt.plot(a,b,'g-', linewidth=2)
    plt.plot(a,b,'b-', markersize=15, alpha=0.75)
    #
    plt.legend(loc="upper left")
    plt.grid(True)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.show()
myDraw()


## 实例2: 做模型验证：验证中心极限定理
import numpy as np
import matplotlib.pyplot as plt
def validateModel():
    #均匀分布
    u=np.random.uniform(0.0, 1.0, 10000)
    plt.hist(u, 80, facecolor='g', alpha=0.75)
    plt.grid(True)
    plt.show()
    #
    # 1万个均匀分布累加/n，很接近正态分布(又叫 高斯分布)
    times=10000
    for time in range(times):
        u += np.random.uniform(0.0, 1.0, 10000)
    print(len(u))
    u /= times
    print(len(u))
    #
    plt.hist(u, 80, facecolor='g', alpha=0.75)
    plt.grid(True)
    plt.show()
#
validateModel()


实例3: 线性回归、rate、Loss 
无代码


实例4: EM code
有1000个身高数据，假定男性身高符合 N(mu1, sigma1^2), 女性身高符合 N(mu2, sigma2^2)，
x个男性和y个女性混合到一起，构成一个高斯混合模型。
可以使用EM模型推测：男女的分布参数mu1,mu2, sigma1,sigma2都是什么，男女比例多少。


实例5: EM 算法: 无监督分类鸢尾花数据
横轴身高、纵轴体重，也可以使用EM算法推测构成，及参数。


等高线图，画的其实是似然函数的折线图。



实例5: 图形前景色、背景色区分



实例6: 图像的卷积。
卷积神经网络 CNN
不同的算子得到不同的结果，然后就可以上分类器了：随机森林，logistic回归。


实例7: 去均值ICA分离
源信号1: 独立成分1
源信号2: 独立成分2
混合信号1: 混合信号2

带噪声的信号分离。


实例8: 高斯核函数的影响
SVM的支撑向量、过渡带
用高斯分布，过渡曲线，则分类效果更精准。


实例9: crawler 爬取数据
HMM分词: MLE，很多做分词的算法

LDA 主题模型分布、词分布

舆情:
	获取QQ群聊天记录: 文本格式
	整理成 QQ号/时间/留言 的规则形式
		- 正则表达式
		- 清洗特定词：表情、@xx
		- 使用停止词库
		- 获得csv表格数据
	合并相同QQ号的留言
		长文档利于计算每人感兴趣话题
	LDA 模型计算主题
		调参与可视化
	计算每个QQ号及众人感兴趣话题
#

实例10: 石油例行检查结果处理
通过主题模型方案，分析例行检查结果中最突出的问题是什么？
文本共4700个
单个文档+数字



实例11：其他内容
- 最大熵模型: 自然语言处理解决标记问题
- 聚类: k-means / k-mediods / 密度聚类 /谱聚类
- 降维: PCA/ SVD/ ICA
- SVM: 与核技术相结合
- 主题模型pLSA/LDA: 与聚类、标签传递算法相结合
- 条件随机场: 无向图模型、链式条件随机场解决标记问题
- 变分推导Variation Inference: 与EM、贝叶斯相结合，参数、隐变量的学习
- 深度学习: 大规模人工神经网络

希望大家在课程结束时，都能搞明白这些。




========================================
|-- 本课程参考文献 (非常重要)
----------------------------------------
- Christopher M Bishop: Pattern Recognition and Machine Learning, Spinger 2006;
	世界上最好的PRML
	缺点: 1太厚了; 2 贝叶斯观点解释问题太多了，公式太多，以至于描述的很复杂。
- Kevin: Machine Learning: A Probabilistic Perspective, MIT 2012
	和PRML互补，补充前者没有的细节
	
- 李航，统计学习方法，清华大学出版社，2012
- Stephen Boyd: Convex Optimization: Cambridge University 2004
	图优化
- Thomas M, Elements of Information Theory, 2006
	信息论
- 各章节特定的经典论文，如: ...



========================================
*** 数学背景 ***
----------------------------------------

1. 回忆级数: 求S的值
S= 累加(n=0,无穷大, 1/n!) =1/0! + 1/1! + 1/2! + 1/3! +...+ 1/n! +... 

## my code: 直接算 
def base0(n):
    arr=[1]
    for i in range(1,n):
        arr.append(arr[-1]*i)
    return(arr)
#
#
num=20
for j in range(1,num):
    arr2=base0(j)
    s=0
    for i in range(len(arr2)):
       s+=1/arr2[i] 
    print(j, s) #看来结果是 自然对数的底 e
#



(2) 对数函数的上升速度
import math
import matplotlib.pyplot as plt
import numpy as np

def test1():
    x=np.arange(0.05, 3, 0.05)
    y1=np.log(x)/math.log(1.5)
    #print(np.log(2.71828), math.log(2.71828))
    plt.plot(x,y1, linewidth=2, color='#007500',label="log1.5(x)")
    #
    plt.plot([1,1], [y1[0], y1[-1]], 'r--', linewidth=2)
    #
    y2=np.log(x)/math.log(2)
    plt.plot(x,y2, linewidth=2, color='#9f35ff',label="log2(x)")
    #
    y3=np.log(x)/math.log(3)
    plt.plot(x,y3, linewidth=2, color='#f75000',label="log3(x)")
    #
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()
test1()
# 底>1时单调递增，越大上升越慢。
# 函数f(x)=log a(x) 一定过(1,0)点。

那么在(1,0)点，也就是x=1时，底a为多少时切线斜率为1呢？
i) my:
log a(x)'=(ln(x)/ln(a))'=1/x /ln(a)=1，带入位置x=1， ln(a)=1, a=e;

ii) 视频: 这里用了大量微积分推导
自然对数 lim(x->无穷大时, (1+1/x)^x )=e;

先是用an=(1+1/n)^n的二项展开，证明{an}数组有上界，该函数还单调递增，则必有极限，记为e.


结论:
	- 导数就是曲线的斜率，是曲线变化的快慢程度的反应。
	- 二阶导数是斜率的变化快慢的反应，表征曲线凹凸性。
		- 二阶导数连续的曲线，往往称之为"光滑"的
		- 高中物理: 加速度的方向总是指向轨迹曲线凹的一侧。
	- 根据 lim(x->无穷大时, (1+1/x)^x )=e 可以得到函数 f(x)=ln(x)的导数，
		进一步根据换底公式、反函数求导等，得到其他初等函数的导数。
#





2. 常用函数的导数
C'=0
(x^n)'=n*x^(n-1)

(sin x)'=cos x
(cos x)'=-sin x 

(tan x)'=1/(cos x)^2
(cot x)'=-1/(sin x)^2

(arcsin x)'=1/sqrt(1-x^2)
(arccos x)'=-1/sqrt(1-x^2)

(arctan x)'=1/(1+x^2)
(arccot x)'=-1/(1+x^2)

(a^x)'=a^x *ln(a)
  (e^x)'=e^x 

(log a x)'=1/x * log a e 
  (ln x)'=1/x 

(u+v)'=u' + v'
(uv)'=u'v + uv'


机器学习为什么用导数？
因为损失函数f(theta)~theta 中的最小值，就可以沿着导数的方向逐步尝试最后找到局部最优解。


注意事项
i.不是所有的函数都可以求导；
ii.可导的函数一定连续，但连续的函数不一定可导（如y=|x|在y=0处不可导）。


(2) 应用
例题1: 已知函数 f(x)=x^x, x>0, 求f(x) 的最小值;
	领会 幂指数函数 的一般处理套路。
# 解:  
y=x^x 两边同时求对数ln
ln y=x ln x 两边同时求x的导数
1/y * y'= 1*ln x + x*1/x=1+ln x
y'=y(1+ln x)=x^x *(1+ln x)=0 时有极值
ln x=-1, x=1/e
# 极值为 x=1/np.e;x**x; 0.6922006275553464

作图发现，最小值在0.4附近
def fn1(x):
    return( x**x )
#fn1(0.01)
def test2():
    x=np.arange(0.00001, 1.5, 0.01)
    y=fn1(x)
    plt.plot(x,y, label="y=x^x")
    plt.plot([1/np.e, 1/np.e], [min(y)-0.1, max(y)], 'm--', linewidth=2) #参考线
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()
test2()



例题2: N^(1/ log2 N)=?
在计算机算法跳跃表 Skip List 的分析中，用到了该常数。
背景: 跳表是支持增删改查的动态数据结构，能够达到与平衡二叉树、红黑树近似的效率，而代码实现简单。

y=x^(1/log2 x) 两边同时取对数
ln y= 1/(log2 x) * ln x=1/(ln x/ ln 2) * ln x=ln 2
y=2 




========================================
积分：分步积分法、泰勒展开
----------------------------------------

例题3: N趋于无穷大时， ln N! -> N(ln N - 1)
ln N! = 求和(i=1, N, ln i) ~约等于 积分(1, N, ln(x)dx)
//积分忘了怎么求了，百度了一下，说分步积分法
=x*ln(x)|(积分从1,N) - 积分(1,N, xd(lnx))
=N*ln(N)-积分(1,N, x*1/x d(x))
=N*ln(N)-x|积分从(1,N)
=N*ln(N)-N+1
趋近于 N(lnN)-N=N(lnN-1)
如果做算法计数，可以更粗糙一点 趋近于 N*ln(N);

结论: 离散情况的加和，近似于连续情况下的积分。
ln N!=求和(i=1, N, ln i) 就是求ln(x)曲线下一系列宽度为1、高度为 ln(i)的矩形面积的和。
近似等于曲线下面积。




例题4: Taylor公式——Maclaurin公式
泰勒展开公式，在任何一点x0处的展开:
f(x)=f(x0)+f'(x0)(x-x0)+f''(x0)/2!*(x-x0)^2+...+f^(n)(x0)/n!*(x-x0)^n + Rn(x)
最后是一个xn的高级无穷小，...? 代数余子式

如果取x0=0，则 
f(x)=f(0)+f'(0)*x+f''(0)/2!*x^2+...+f^(n)(0)/n!*x^n+o(x^n)
这个又叫 Maclaurin公式(麦克劳林公式)


泰勒公式的应用
(1) 数值计算: 初等函数值的计算(在原点展开)
e^x = 1 + x + x^2/2! + x^3/3! +...+ x^n/n! + Rn;
当x=1时，e=2+1/2!+...+1/n!+...

sin(x)=x-x^3/3!+x^5/5!-x^7/7!+x^9/9!+...+(-1)^(m-1)*x^(2*m-1)/(2*m-1)!+R2m;

(2)在实践中，往往需要做一定程度的变换。
无穷级数
3.2=3+0.2
4.8=5-0.2
任何实数都可以写成整数和绝对值<=0.5的小数和的形式 
	x=N+alpha, 其中 abs(alpha)<0.5
#


#############
泰勒公式的应用 例题1: 给定实数x，计算e^x=?
写成ln2进制形式 x=(ln2)*K+r, 其中k是整数，r是实数且abs(r)<=0.5*ln2，非常接近1;
e^x = e^( (ln2)*k+r ) = e^((ln2)*k) * e^r=2^k * e^r  
	因为 e^ln2=2; 而e^r则接近e;
	e^x约等于2^k，二进制移位运算很快的?
#
按照这个 数值分析，也可以近似计算sin(x)，仅依赖基本的四则运算。




#############
泰勒公式的应用 例题2: 考察Gini系数的图像、熵、分类误差率三者之间的关系。
# 将 f(x)=-ln(x)在x=1处一阶展开，忽略高阶无穷小，得到 f(x)~1-x
H(x)=-求和(k=1, K, pk*ln(pk))约等于 求和(k=1, K, pk(1-pk))

上述结论，在决策树章节中进一步讨论。

代码: 
import numpy as np
import matplotlib.pyplot as plt

# if __name__=="__main__":
p=np.arange(0.001, 1, 0.001, dtype=np.float)
gini=2*p*(1-p)
h=-(p*np.log2(p) + (1-p)*np.log2(1-p))/2
err=1-np.max(np.vstack((p, 1-p)), 0) # vstack是啥？
plt.plot(p, h, 'b-', linewidth=2, label="Entropy")
plt.plot(p, gini, 'r-', linewidth=2, label="Gini")
plt.plot(p, err, 'g-', linewidth=2, label='Error')
plt.grid(True)
plt.legend(loc="upper left")
plt.show()








========================================
|-- 方向导数、梯度、梯度下降法
----------------------------------------
#############
方向导数
定义: 如果函数z=f(x,y)在点P(x,y)是可微分的，那么，函数在该点沿任一方向L的方向导数都存在，且有：
df/dL=df/dx * cos(fai) + df/dy * sin(fai) #d表示偏导数
其中fai为x轴到方向L的转角。

换一种写法
(1) 引入了 梯度 的概念。
设函数z=f(x,y)在平面D内具有一阶连续偏导数，则对于每个点P(x,y)属于D, 向量(df/dx, df/dy)为函数z=f(x,y)在点P的梯度，记做 grad(f(x,y))。

梯度的方向是函数在该点变化最快的方向。是局部最优的。
 - 和一维沿着导数方向变化最快一样，沿着这个方向变化最快。
 - 考虑一座解析式为z=H(x,y)的山，在(x0,y0)的梯度是该点坡度变化最快的方向。

(2) 梯度下降法：
思考：若下山方向和梯度呈theta夹角，下降速度是多少？

对上述梯度向量做线性变换: (cos(fai), sin(fai))T，就是右乘到上述向量，得:
(df/dx, df/dy) . (cos(fai), sin(fai))T 向量的点乘，就是df/dL

向量的乘法，a.b=|a|*|b|*cos(a,b夹角) 在夹角为0时乘积最大。
也就是想要最快速下降，就要沿着梯度方向走。

举例: 对于损失函数 y=x^2, 导数是y'=2*x, 
则在x=3开始梯度下降，
假设 alpha=0.1时
y'=2*x, xNew=xOld-y'*alpha, y=x^2; 直到得到y的极小值。
x=3, 损失函数 y=9
y'=6, x=3-6*0.1=2.4, y=5.76
y'=4.8, x=2.4-4.8*0.1=1.92, y=3.6864
...

#



========================================
|-- Gamma函数: 阶乘在实数上的推广
----------------------------------------
如果阶乘能推广到实数，也就是对任意实数x，满足性质：
Gamma(x)=(x-1)*Gamma(x-1), 也就是 G(x)/G(x-1)=x-1

那么Gamma公式是什么样的呢？(Gamma函数由22岁的欧拉于1720年发现。)
Gamma(x)=积分(0, +无穷大, t^(x-1)*e^(-t)*dt)=(x-1)!


证明过程: 
https://blog.csdn.net/pilotmickey/article/details/105345457
Gamma(n)=积分(0, +无穷大, x^(n-1) * e^-x) dx
= - 积分(0, +无穷大, x^(n-1) ) de^-x   #使用分部积分法
= -x^(n-1)*e^-x|() + 积分(0,+无穷大, e^-x * (n-1)*x^(n-2) * dx) 
= -x^(n-1)*e^-x|() + (n-1)*积分(0,+无穷大, e^-x * x^(n-2) * dx) 
= -x^(n-1)*e^-x|() + (n-1)*Gamma(n-1)
### 很容易求得 -x^(n-1)*e^-x|(0, +无穷大)=0
=(n-1)*Gamma(n-1)




使用py绘制阶乘: 2.18!
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from scipy.special import gamma
from scipy.special import factorial

#mpl.r(Params['axes.unicode_minus'])=False
#mpl.r(Params['font.sans-serif'])='SimHei'

N=5
x=np.linspace(0,N,50)
y=gamma(x+1)
plt.figure(facecolor='w')
plt.plot(x,y,'r-', lw=2)

z=np.arange(0,N+1)
f=factorial(z, exact=True)
print(f)
plt.plot(z, f, 'go', markersize=8)
plt.grid(b=True)
#plt.xlim(-0.1, N+0.1)
#plt.ylim(0.5, np.max(y)*1.05)
plt.xlabel('x', fontsize=15)
plt.ylabel('Gamma(X) ~ n!', fontsize=15)
plt.title("Gamma(x) and n!")
plt.show()

#######
# 更精细的尺度: gamma函数先降低，后升高
N=2
x=np.linspace(0,N,50)
y=gamma(x+1)
plt.figure(facecolor='w')
plt.plot(x,y,'r-', lw=2)

z=np.arange(0,N+1)
f=factorial(z, exact=True)
print(f)
plt.plot(z, f, 'go', markersize=8)
plt.grid(b=True)
#plt.xlim(-0.1, N+0.1)
#plt.ylim(0.5, np.max(y)*1.05)
plt.xlabel('x', fontsize=15)
plt.ylabel('Gamma(X) ~ n!', fontsize=15)
plt.title("Gamma(x) and n!")
plt.show()





========================================
|-- 凸函数 (割线位于函数值的上方。下凸函数)
----------------------------------------
1.定义 
若函数f的定义域domf为凸集，且满足
对任意x,y属于dom f, 1<=theta<=1, 有
f(t*x + (1-t)*y) <= t*f(x) +(1-t)*f(y)

直观理解，就是 割线位于函数值的上方，这样的函数式凸函数。





2. 性质： 一阶可微
若f一阶可微，则函数f为凸函数当且仅当f的定义域dom f为凸集，且 
任意x,y属于dom f, f(y)>=f(x)+梯度f(x)^T * (y-x)


二阶可微 
若函数f二阶可微，则函数f为凸函数当且仅当dom f为凸集，且 梯度^2f(x) >=0

若f是一元函数，上式表示二阶导数大于等于0;
	比如 y=x^2, 一阶导数y'=2*x, 二阶导数y''=2>0;
若f是多元函数，上式表示二阶导数Hessian矩阵半正定。
	比如 f(x,y)=2*x^2 + 3*y^2 - x*y
	求偏导数 
		df/dx=4x-y; 
			再对x求导 d^2f/d^2x=4
			再对y求导 d^2f/(dx dy)=-1; 
		df/dy=6y-x; 
			再对x求导 d^2f/(dx dy)=-1; #连续函数交换求导顺序，值不变(??)
			再对y求导 d^2f/d^2y=6;
整理成矩阵
		二阶x  二阶y
一阶x	4 -1
一阶y	-1 6
这个二阶导数构成的2x2的方阵就是Hessian(海瑟)矩阵。

什么是正定矩阵？
怎么判定正定矩阵？每个主余子式都是正的。 H1=4>0; |H|=23>0; 所以该矩阵式正定的。

正定就是凸函数。





3. 凸函数举例

指数函数 f(x)=e^(ax)
幂函数 f(x)=x^a, x属于正实数， a>=1或a<0
负对数函数 f(x)=-ln(x)
负熵函数 f(x)=x*ln(x)
范数函数 f(x_bar)=||x||
最大值函数 f(x_bar)=max(x1,x2,...,xn)
指数线性函数 f(x_bar)=log(e^x1 + e^x2+...+e^xn)
	log sum exp=max(x1,x2,...,xn)
#


z=max(x,y) 漏斗形的，就是一个倒置的金字塔

f(x1,x2)=log(e^x1+e^x2) 的曲线什么样的呢？
这其实是对max(x,y)的软近似: softmax
	soft-max 回归 ??
	类似的有 logistic回归：一个重要的分类器
# 可视化代码
fig=plt.figure()
ax=fig.add_subplot(111)
u=np.linspace(0, 4, 1000)

x,y=np.meshgrid(u,u) #??
z=np.log(np.exp(x) + np.exp(y))
ax.contourf(x, y, z, 5) #最后一个参数表示渐变精细程度:3 粗糙, 20好一点，50更精细
plt.show()






========================================
概率论：古典概率模型(151min)
----------------------------------------
X离散，P(X=x0)表示x0发生的概率;
X连续，P(X=x0)表示x0发生的概率密度;


缩写词:
累计分布函数: CDF; 大写
概率密度函数: pdf; 小写


1. 古典概率模型:
n个不同的球放到N(N>=n)个不同的盒子中，假设盒子容量无限，求事件A={每个盒子最多一个球}的概率。
p=A(n,N) / N^n;


例题1:生日悖论：
班里50个同学，至少2人生日相同的概率是多少？
解: 相当于50个球放到365个桶里 365^50
每个桶一个球的概率 A(365, 50)
p=1-A(365,50)/365^50=0.97 
悖论：虽然50个365距离挺远的，但是貌似


f(n)=1-A(365,n)/365^n，其中n是总人数。


## 使用R语言计算概率：
#排列数
P=function(N,K){ choose(N,K) * factorial(K) }
#
fn1=function(x){
  1-P(365,x)/365^x
};
fn1(10) #0.1169482
fn1(20) #0.4114384
fn1(30) #0.7063162
fn1(40) #0.8912318
fn1(50) #0.9703736






例题2: 装箱问题
12件正品和3件次品，随机装到3个箱子中，每个箱子5件，
则每箱中恰好有1件次品的概率是多少？

策略: 正难则反。

总共的放法: 隔板法，15个小球，放了3个隔板，每个隔板内部随意组合
15!/5!/5!/5!

每个箱子一个次品的放法：每个箱子里一个次品 A(3,3)，然后其余12个小球隔板法放3个隔板，内部随意组合 12!/4!/4!/4!

概率为 A(3,3)*12!/4!/4!/4! / ( 15!/5!/5!/5! )=25/91 = 0.2747253





与组合数的关系
(1) 把n个物品分成k组，使得每组物品个数分别是n1,n2,...,nk，其中n=n1+n2+...+nk,则不同的分组方法有 n!/(n1!n2!...nk!)种。
(2) 上述问题简化版本，n个物品分成2组，第一组m个，第二组n-m个，则分组方法有
n!/(m!(n-m)!)，也就是组合数 C(n,m)


(3) 问题(1) 取对数再除以n之后是什么？
H=1/n*ln( n!/(n1!n2!...nk!) )=1/n*[ ln n! - ln 连乘(i=1,k, ni!) ]
回忆上面的近似 ln(n!) -> n(ln(n) -1)
-> 1/n[ n(ln(n)-1) - 求和(i=1, k, ln(ni!)) ]
-> 1/n[ n(ln(n)-1) - 求和(i=1, k, ni*(ln(ni)-1) ) ]
=ln(n)-1 - 1/n*求和(i=1, k, ni*(ln(ni)-1) )
-> ln(n) - 1/n*求和(i=1, k, ni*ln(ni) ) 
= -1/n*[求和(i=1, k, ni*ln(ni)) -n*ln(n)  ]
= -1/n*[求和(i=1, k, ni*ln(ni)-ni*ln(n) )  ]
= -1/n*[求和(i=1, k, ni*(ln(ni)-ln(n)) )  ]
= -1/n*[求和(i=1, k, ni*(ln(ni/n)) )  ]
= -[求和(i=1, k, ni/n*(ln(ni/n)) )  ]
-> -[求和(i=1, k, pi*ln(pi)] 这就是熵。

其中 ni/n 就是概率pi，表示第i个盒子内装ni个球的概率。


(1)熵是混乱程度的反应。越平均越混乱。
1 0: -1*log(1)=0;
fn=function(p){
	q=1-p;
	-p*log(p)-q*log(q)
}
0.1, 0.9: -0.1*log(0.1)-0.9*log(0.9)=fn(0.1)=0.32508
0.3, 0.7: fn(0.3)=0.6108
0.5, 0.5: fn(0.5)=0.6931 #分两份时，最平均的熵最高
0.7, 0.3: fn(0.7)=0.6108
#
同理，分三份的，也是最平均时的熵最高。

(2)熵用于决策树，随机森林。


参考文献:
1. Andrew Ng, machine Learning, Stanford University;
2. 数学随便找一本大学教科书: 同济大学数学教研室，高等数学，高等教育出版社。
3. 概率论和数理统计






# 布封投针实验
平面上画距离为a的若干平行线，将长度为L的针随机丢到桌面上，
则这根针与平行线相交的概率是多少？
假定L<=a;

布封本人证明了 p=2*L/pi/a, 其中pi为圆周率。
推导过程: https://blog.csdn.net/wy_97/article/details/103448816
t是针和线的角度，则针中点到线的距离 x<= L/2*sin(t) 时相交;
t范围[0, pi]
x<=L/2<=a/2,范围[0, a/2]
S(g)=L/2*2=L;
S(G)=pi*a/2;
p=S(g)/S(G)=2*L/(pi*a)
也就是说模拟的次数足够多时，可以由p求出pi
pi= 2*L/(p*a), L<=a;


使用程序模拟，不知道怎么写...
抄的 https://blog.csdn.net/qq_36306781/article/details/81280210

# n 投针次数
# a 平行线间距
# L 针长度，且L<=a
buffon<-function(n,a,L){
  m<-0
  for (i in 1:n) {
    x<-runif(1)*a #随机化起点[0, a]
    theta<-runif(1)*pi #随机化角度 [0, pi]
    if(L*sin(theta)>=(a-x) ){m=m+1} 
  }
  #估算概率
  p<-m/n
  #估算pi
  pie<-2*L/(a*p)
  result<-c('p.e'=p,'pi.e'=pie);result
}
set.seed(1)
buffon(10000,1,0.8) #3.195526 
buffon(10000,2,1.5) #3.16723
buffon(10000,3,2) #3.14911
#
set.seed(202007)
p1=c()
for(i in 1:100){
  if(i %% 10==0)print(i)
  p1=c(p1, buffon(1e4,1,0.8)[2])
}
head(p1)
hist(p1, n=20)
mean(p1) #3.137111

> buffon(1000000,1,0.8)
     p.e     pi.e 
0.509118 3.142690 




========================================
|-- 概率论与贝叶斯先验
----------------------------------------
主要内容

概率论基础
	概率与直观
	常见概率分布
	sigmoid/logistic函数的引入
统计量
	期望、方差、协方差、相关系数
	独立和不相关
大数定理
中心极限定理
最大似然估计
	过拟合
#


1. 统计数字的概率
(1)
题目: 给定某个正整数N，统计从1!,2!,..., N! 中，首位数字出现1的概率。
进而，可以计算首位数字是2的概率，是3的概率，...，从而得到一条"九点分布"。

我们猜测可能机会均等，那么每个数字出现的概率都是1/9？

我们使用py模拟一下试试。

def first_digital(x):
    x=str(x)
    x=x[0:1]
    return( int(x) )
first_digital(29909)

def demo1(N):
    n=1
    frequency=[0]*9
    for i in range(1, N):
        n *=i
        m=first_digital(n)-1
        frequency[m]+=1
    print(frequency)
    x=[x for x in range(1,10)]
    plt.plot(x,frequency, 'r-', linewidth=2)
    plt.plot(x, frequency, 'go', markersize=8)
    plt.grid(True)
    plt.title(str(N-1)+"! init number freq")
    plt.show()
demo1(N=101)
demo1(N=1001)
demo1(N=10001) #太慢了，就到这里吧。
## 
aa=[2956, 1789, 1276, 963, 794, 715, 571, 510, 426]
s1=sum(aa)
[x/s1*100 for x in aa]
[29.56,
 17.89,
 12.76,
 9.629999999999999,
 7.9399999999999995,
 7.1499999999999995,
 5.71,
 5.1,
 4.26]
#



本福特定律(本福特法则， Frank Benford)，又称第一数字定律，
是指在实际生活得到的一组数据中，以1为首位数字的概率约为总数的三成。
是直观想象1/9的三倍。
- 阶乘/素数数列/斐波那契数列首位
- 住宅地址号码
- 经济数据反欺诈
- 选举投票反欺诈





(2)商品推荐
商品推荐过于集中，会伤害用户体验。通过引入随机性，给用户带来惊喜感。
假设在某推荐场景中，商品A和B与当前访问用户的匹配度分别为0.8和0.2分，系统为A随机生成一个均匀分布于[0,0.8]的最终得分，为B生成一个均匀分布[0,0.2]的最终得分，试计算最终得分B大于A的概率？

理论上，A得分的期望是 0.8， B是0.2，则A大于B是大概率事件。
通过画图解答，x坐标为A[0, 0.8]，y坐标为[0, 0.2],画一条线A=B，则求曲线上区域的面积。
曲线上是三角形 0.2*0.2/2=0.02;
整个矩形的面积 0.2*0.8=0.16
p=0.02/0.16=0.125;







========================================
|-- 概率公式与贝叶斯公式
----------------------------------------
1. 条件概率
P(A|B)=P(AB)/P(B)

推导: 根据定义，AB同时发生的概率，就是A发生的概率*A发生的条件下B发生的概率。
P(AB)=P(A)*P(B|A)
同理，有 P(AB)=P(B)*P(A|B)


2. 全概率公式
P(A)=求和(i, P(A|Bi)*P(Bi))

对全集做不重不漏的切割B1,B2,...,Bn;
则每种情况Bi下A发生的概率P(A|Bi)的和等于P(A).


3. 贝叶斯(Bayes)公式: 应用条件概率和全概率公式推出来的。
P(Bi|A)=P(A|Bi)*P(Bi)/求和(j, P(A|Bj)*P(Bj) )

推导
P(AB)=P(A)*P(B|A)=P(B)*P(A|B)
得 P(B|A) = P(A|B)*P(B) / P(A)




例题:
8支步枪中有5支已校准过，3支没有校准过。
一个射手射击，使用校准过的枪中靶的概率是0.8，使用未校准过的枪中靶的概率是0.3；
现在随机挑选一支枪，中靶了。求使用的是已经校准过的枪的概率？
解: 对事件做2次划分。
是否校准过: 未校准 P{A=0}=3/8，校准 P{A=1}=5/8;
是否中靶: B=0 未中，B=1中了。
p{B=1|A=0}=0.3; 
p{B=1|A=1}=0.8; 
求 p{A=1|B=1}=?

p{A=1|B=1} p{B=1|A=1}*p{A=1} / p(B=1)
= p{B=1|A=1}*p{A=1} / ( p{B=1|A=0}*P{A=0}+p{B=1|A=1}*P{A=1} )
=0.8*5/8 / (0.3*3/8+0.8*5/8)=4/(0.9+4)=0.81632




贝叶斯公式 P(B|A) = P(A|B)*P(B) / P(A)
给定某系统的若干样本x，计算该系统的参数，即
p(theta|x)=p(x|theta)*p(theta)/p(x)

- p(theta) 没有数据支持下，theta发生的概率：先验概率。
- p(theta|x) 在数据x的支持下，theta发生的概率: 后验概率。
- p(x|theta): 给定某参数theta的概率分布：似然函数。

例如:
- 在没有任何信息的前提下，猜测某人姓，先猜"李王张刘..."猜对的概率相对较大，先验概率。 
- 如果知道某人来自"大王庄"，则姓王的概率很大：后验概率。但是不排除其他姓氏的可能。




========================================
|-- 2点分布、二项分布
----------------------------------------
1. 0-1分布，已知随机变量X的分不律为 
X 1 0
P p 1-p 
则 EX=1.p+0.q=p;
D(X)=E(X^2)-(EX)^2=1^2.p+0^2.q-p^2=pq;




2. 两点分布 伯努利分布
随机变量X服从参数为n,p的二项分布。

法1: 设Xi为第i次试验中事件A发生的次数，i=1,2,...,n,
则 X=累加(i=1, n, Xi)

显然，Xi相互独立均服从参数为0-1分布，
所以 EX=累加(i=1, n, E(Xi))=np;
DX=累加(i=1, n, DXi)=npq;

法2: 使用排列组合公式计算
p(k)=C(n,k)*p^k*(1-p)^(n-k), k=0,1,2,...,n;
EX=...=np;
E(X^2)=...=(n^2-n)*p^2 + np;
DX=E(X^2)-(EX)^2=npq;



还有 多项分布，比如小说分类：武侠、言情、悬疑、恐怖、...









========================================
|-- Taylor展开式到泊松分布
----------------------------------------
1. 考察Taylor展开式
e^x=1 + x + x^2/2! + x^3/3! + ... + x^k/k! + Rk

两边同除以e^x，得
1=exp(-x) + x*exp(-x) + x^2/2!*exp(-x)+...+x^k/k!*exp(-x)+Rk*exp(-x)

则通项公式为 x^k/k!*exp(-x)
换x为Lambda则为 L^k/k!*exp(-L)

且和为1，正好是一个概率分布，这就是 泊松分布！



2. 设X~PI(Lambda)，且分布律为
P{X=k}= L^k/k!*exp(-L), k=0,1,2,...
则有 EX=累加(k=0, 无穷大, k*p)
=累加(k=0, 无穷大, k*L^k/k!*exp(-L))
=exp(-L)*累加(k=1, 无穷大, L^(k-1)/(k-1)!*L)
=L*exp(-L)*exp(L)=L;


E(X^2)=E[X(X-1)+X]=E[X(X-1)] +X(X)
=L+ 累加(k=0, 无穷大, k*(k-1)*L^k/k!*exp(-L) )
=L+ L^2*exp(-L)*累加(k=2, 无穷大, L^(k-2)/(k-2)!)
=L+ L^2*exp(-L)*exp(L)
=L+L^2;

DX=E(X^2)-(EX)^2=L;



那么，泊松分布的Lambda是什么意思呢？
P{X=k}=L^k/k!*exp(-L);
可以理解为 L是某一个rate, 除以 exp(L) 后的累加和才是1.
如果不除以 exp(L)，则累加和为exp(L)
e^x=1 + x + x^2/2! + x^3/3! + ... + x^k/k! + Rk
X越大，右边每一项也越大。


在实际事例中，当一个随机事件，以固定的平均瞬时速度Lambda或称密度随机且独立地出现时，那么，这个事件在单位时间(或面积、体积)出现的次数或个数就近似地服从泊松分布P(Lambda).
- 汽车站台的人数
- 机器发生的故障数



========================================
|-- 均匀分布
----------------------------------------
1. 均匀分布
设X~U(a,b)，其概率密度为
f(x)=1/(b-a), a<x<b,
f(x)=0, 其他;

则有 EX=积分(-无穷大, +无穷大, x*f(x)dx)
=积分(a, b, x*f(x)dx)=(a+b)/2

E(X^2)=积分(a,b, x^2/(b-a)dx) -(EX)^2
=1/(b-a)*1/3*(b^3-a^3) -(a+b)^2/4
=1/(b-a)*1/3*(b-a)*(b^2+ab+a^2) -(a+b)^2/4
=(b^2+ab+a^2)/3-(a^2+2ab+b^2)/4=(b-a)^2/12;





========================================
|-- 指数分布
----------------------------------------
2. 指数分布 
(1)
积分(-无穷大, 0, exp(x) dx)=exp(x)|(-无穷大,0)=1-0=1

取负号，也就是 exp(-x)在 (0, +无穷大)上积分为1, 是一个连续的分布。
更一般的，也可能对x除以个theta，或者乘以一个labmda，是一个意思。

概率密度函数为: theta>0
f(x)=1/theta*exp(-x/theta), x>0, 
f(x)=0, x<=0;

数字特征
EX=积分(-无穷大, +无穷大, x*f(x) dx)
=积分(0, +无穷大, x/theta*exp(-x/theta) dx)
=-x*exp(-x/theta)|(0,+无穷大) + 积分(0,+无穷大, exp(-x/theta) dx)
=0-theta*exp(-x/theta)|(0, +无穷大)
=theta

DX=E(X^2)-(EX)^2=积分(0, +无穷大, x^2/theta*exp(-x/theta) dx) -theta^2
=2*theta^2 - theta^2= theta^2;



(2) 指数分布另一种形式
概率密度函数为: labmda>0
f(x)=labmda*exp(-labmda*x), x>0, 
f(x)=0, x<=0;

labmda>0 是分布的一个参数，叫 rate parameter 率参数，
	表示 每单位时间内发生某事件的次数。
指数分布的区间是[0, 无穷大)
如果一个随机变量X呈指数分布，则可以写作 X~Exponential(Lambda)


(3) 意义
指数分布可以表示独立随机事件发生的时间间隔。
比如旅客进机场的时间间隔、软件更新的时间间隔等。

许多电子产品的寿命分布一般服从指数分布。系统可靠性研究中最常用。



(4) 指数分布的无记忆性
指数分布的一个重要特征是无记忆性(遗失记忆性, Memoryless Property)

如果一个随机变量呈指数分布，当s,t>=0时有:
P(x>s+t | x>s)=P(x>t)
即，如果x是某电器元件的寿命，已知元件使用了s小时，则共使用至少s+t小时的条件概率，与从未使用开始至少使用t小时的概率相等。

一个使用100天的二极管，和一个全新的二极管，能再使用10天的概率是相同的。

这和我们的经验是不一定吻合的。
如果是灯泡，则无法说服我们相信它符合指数分布，就需要加一个衰减因子。




思考: 是否有"半记忆性"?
马尔科夫模型，是半记忆性的？




========================================
|-- 正态分布, 二元正态分布
----------------------------------------
1. 设X~N(mu, sigma^2)，其概率密度为
f(x)=1/sqrt(2*PI)/sigma*exp( -(x-mu)^2/2/sigma^2 ), sigma>0, -无穷大<x<+无穷大.

则有EX=积分(-无穷大, +无穷大, x*f(x)dx)
=积分(-无穷大, +无穷大, x/sqrt(2*PI)/sigma *exp( -(x-mu)^2/2/sigma^2 )dx )=...=mu;
DX=E(X^2)-(EX)^2=...=sigma^2;


令t=(x-mu)/sigma, 则 x=mu+sigma*t;
t~N(0, 1)
...




2. 多元高斯分布的画法
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from mpl_toolkits.mplot3d import Axes3D #很重要，没有则报错

from matplotlib import cm # should work. Or import matplotlib.cm as cm
# cm 是啥？

def test():
	x1,x2=np.mgrid[-5:5:51j, -5:5:51j]
	x=np.stack( (x1, x2), axis=2)
	
	plt.figure(figsize=(9,8), facecolor='w')
	sigma=(np.identity(2), np.diag((3,3)), np.diag((2,5)), np.array( ((2,1), (2,5)) ))
	for i in np.arange(4):
		ax=plt.subplot(2,2,i+1, projection='3d')
		norm=stats.multivariate_normal((0,0), sigma[i])
		y=norm.pdf(x)
		ax.plot_surface(x1,x2, y, cmap=cm.Accent, rstride=2, cstride=2,alpha=0.9,lw=0.3)
		ax.set_xlabel(u'x')
		ax.set_ylabel(u'y')
		ax.set_zlabel(u'z')
	plt.suptitle(u'Var of bivariate Gaussian distribution', fontsize=18) #二元高斯分布
	plt.tight_layout(1.5)
	plt.show()
test()

对于二元高斯分布(x,y), 
均值是分别计算的(mux, muy), 比如(0,0)
而方差，则是主对角线是方差，其他位置为协方差的 协方差矩阵是2x2的:
[sigmaxx^2, sigmaxy^2]
[sigmayx^2, sigmayy^2]

比如 diag(3,3) 就是协方差矩阵为 
[3 0]
[0 3]

方差=累加(i, (x-x_bar)^2 )= 累加(i, (x-x_bar)*(x-x_bar) )
而两个变量之间的协方差    = 累加(i, (x-x_bar)*(y-y_bar) )






========================================
|-- Beta 分布: 研究概率本身的分布的
----------------------------------------
1. 引入
抛硬币A，正面向上的概率是0.2,
抛硬币B，正面向上的概率是0.3,
抛硬币C，正面向上的概率是0.7,
...
不同的硬币可以认为不同的系统。

...


2. 数学定义
f(p)=p^(a-1) * (1-p)^(b-1) / S(a,b)

Beta分布的概率密度
Beta(x)=1/B(a,b) * x^(a-1) * (1-x)^(b-1), 当x范围[0,1]
Beta(x)=0, 当x范围其他
a和b是参数。

# 其中系数B为S(a,b)是曲线 p^(a-1) * (1-p)^(b-1) 下的面积。
除以总面积就能保证概率和为1。
--> 按照这个思路，可以构造任意连续分布律了？随便给个非负函数，然后除以自己的总面积。

也就是 积分(0,1, x^(a-1) * (1-x)^(b-1)dx )=G(a)G(b)/G(a+b)
其中G()是Gamma函数，可以看做阶乘在实数域的推广:
G(x)=积分(0,1, t^(x-1) * exp(-t)dt)
所以 G(n)=(n-1)!
所以 B(a,b)=G(a)G(b)/G(a+b)




3. Beta分布的期望和方差
(1)根据定义 
f(x)=1/B(a,b)*x^(a-1)*(1-x)^(b-1), x属于[0,1]
B(a,b)=积分(0,1, x^(a-1)*(1-x)^(b-1)dx)=G(a)G(b)/B(a+b)

E(X)=积分(0,1, x*f(x)dx)=1/B(a,b)*积分(0,1, x*x^(a-1)*(1-x)^(b-1)dx)
=B(a+1,b)/B(a,b)=G(a+1)G(b)/G(a+1+b)*G(a+b)/G(a)/G(b)
=a/(a+b)

用py画出Beta分布的函数图形，参数a,b分别为 //todo
1,1
3,2
4,2
4,3
5,3





========================================
|-- 指数族 The exponential family 分布: sigmoid 函数
----------------------------------------
(2) 指数族 The exponential family
f(x)>0, 带入公式 x=exp(ln(x))得 f(x)=exp(ln(f(x)))
也就是任意一个公式，都可以伪装成指数分布的形态
	C*exp(ln(f(x))+B)
p(y:ita)=b(y)*exp(ita^T.T(y)-a(ita))
其中 
- ita叫做该分布的自然参数，或者 canonical parameter;
- T(y) 叫做 sufficient statistic;
	比如高斯分布；
	但是高斯分布混合到一起，形成混合高斯分布，就不是指数族分布;
- a(ita) 是log partition function;
- exp(-a(ita)) 是标准化常数，保证p(y:ita)求和或对y积分为1；
- 如果一个分布只有一个峰值，很有可能是指数族分布；
	但是如果有多个峰值，肯定不是指数族分布；
#


2) 比如: 伯努利分布和高斯分布
x是自变量，y是因变量，我们接下来用y
p=p, y=1;
p=1-p, y=0;
上面2条可以简写为 p^y*(1-p)^(1-y)
=exp( ln(p^y*(1-p)^(1-y)) )
=exp( y*ln(p)+(1-y)*ln(1-p) )
=exp( y*(ln(p)-ln(1-p)) + ln(1-p))
=exp( y*ln(p/(1-p)) +ln(1-p) )
其中可以认为 ln(p/(1-p))=T(p), ln(1-p)=a(p)
到这一步已经证明了伯努利分布就是一个指数族分布。


3)# 接下来继续推导
令fai=ln(p/(1-p))=f，则 exp(f)=p/(1-p)
exp(f)-p*exp(f)=p
p=exp(fai)/(1+exp(fai))
上下同除以分子
p=1/(1+exp(-fai)), 这个就是sigmoid函数，在深度学习中经常用到的激活函数。


4) #注意在推导过程中，出现了 logistic方程
p=1/(1+exp(-ita))
#
画sigmoid/logistic函数图象，y=1/(1+exp(-x)) 就是一个S型曲线，x取值为实数，y轴取值范围 (0,1)
- S 型在英语中叫做sigmoid。
- 用于把x从正负无穷大，映射到[0,1]
- 注意：这个公式左边的p就是上面的伯努利分布的概率p。

5) #求sigmoid函数的导数 
f(x)=1/(1+exp(-x))
f'(x)=(exp(-x))/(1+exp(-x))^2=1/(1+exp(-x)) * exp(-x)/(1+exp(-x))
=1/(1+exp(-x)) * (1-1/(1+exp(-x)))
=f(x)*(1-f(x))








(3) 高斯分布也属于指数族分布
p(y:mu)=1/sqrt(2*pi) * exp(-(y-mu)^2/2)
=1/sqrt(2*pi) * exp(-y^2/2) * exp(mu*y-mu^2/2)
其中 
- ita=mu;
- T(y)=y;
- a(ita)=mu^2/2=ita^2/2;
- b(y)=1/sqrt(2PI)exp(-y^2/2)


以上模型都是线性模型，LM；
或者广义线性模型：GLM;


========================================
|-- *** 事件的独立性
----------------------------------------
1. 事件A和B，如果有 P(AB)=P(A)P(B)，则称事件A和B相互独立。
说明:
- 如果A和B独立，则P(A|B)=P(A)，也就是A的发生和B无关，
	带入定义式 P(AB)=P(A|B)*P(B), 得 P(AB)=P(A)P(B)
- 实践中往往根据2个事件是否互相影响而判断独立性：
	如给定M个样本、若干次采样等情形，往往假定它们互相独立;
	* 独立则：联合概率=边缘概率的乘积。
- 思考: 试给出A、B相互包含的信息量的定义I(A,B)，要求
	如果A和B独立，则I(A,B)=0;
#



========================================
|-- 期望和方差、协方差、相关系数
----------------------------------------
2. 期望，即：概率加权下的"平均值"
- 离散型 E(X)=求和(i, xi*pi)
- 连续型 E(X)=积分(-无穷大,+无穷大, x*f(x)dx)

# 期望的性质
- E(kX)=kE(X)
- E(X+Y)=E(X)+E(Y) #即使X和Y不独立，也成立。

- 如果X和Y相互独立，则 E(XY)=E(X)E(Y)
	反之不成立。如果 E(XY)=E(X)E(Y) 只能说明X和Y不相关。
	* 接着区分 不相关 和 独立的区别。
#



例1: 计算期望
从1,2,3,...,98,99,1024这100个数中任意选取若干个(可能为0个数)求异或，试求异或的期望值。
1) 什么是异或
exclusive OR: 
如果a、b两个值不相同，则异或结果为1。
如果a、b两个值相同，异或结果为0。

异或也叫半加运算，其运算法则相当于不带进位的二进制加法：二进制下用1表示真，0表示假，则异或的运算法则为：0⊕0=0，1⊕0=1，0⊕1=1，1⊕1=0（同为0，异为1），这些法则与加法是相同的，只是不带进位，所以异或常被认作不进位加法。

异或(^)【同0异1】 运算规则:0^0=0;0^1=1;1^0=1;1^1=0; 

2) 看不懂怎么算的，过了 //todo
答案是: 575.5



例2: 集合Hash问题
某Hash函数将任一字符串非均匀映射到正整数k，概率为 2^-k。
现有字符串集合S，其元素经过映射后，得到的最大整数为10,。
试估计S的元素个数。
P{Hash(<string>)=k}=2^-k, k是正实数。

解: 由于Hash映射成整数是指数级衰减的，"最大整数为10"这一条件可近似考虑成"整数10曾经出现"，继续近似成"整数10出现过一次"。

字符串被映射成10的概率为p=2^-10=1/1024,
从而，一次映射即两点分布
p(X=1)=1/1024;
p(X=0)=1023/1024;

而n个字符串的映射，即二项分布:
P{X=k}=C(n,k) *p^k* (1-p)^(n-k),其中p=1/1024;

二项分布的期望是 E(P{X=k})=np，其中 p=1/1024;
而期望表示n次事件发生的次数，当前问题中发生了1次，从而
np=1, n=1/p=1024;


# 注：以上2个例题是阿里考题中比较难的了。





3. 方差 
定义 Var(x)=E([x-EX]^2)=E(X^2)-(EX)^2

性质:
E([x-EX]^2)>=0 等价于 E(X^2)>=(EX)^2 
#当X为定值时，X取等号。


无条件成立 Var(c)=0;
	Var(X+c)=Var(X)
	Var(kX)=k^2*Var(X)
#
X和Y独立: Var(X+Y)=Var(X)+Var(Y)
	此外，方差的平方根，称为标准差。
#






4. 协方差, 协作求和期望差的乘积

注意方差 D(X)= E( (X-EX)*(X-EX) ) #方差是一种特殊的协方差X=Y;

协方差定义 Cov(X,Y)=E( (X-EX)*(Y-EY) )
性质:
	Cov(X,Y)=Cov(Y,X)
	Cov(a*X+b, c*Y+d)=a*c*Cov(Y,X)
	Cov(X1+X2, Y)=Cov(X1,Y)+Cov(X2,Y) #分配率
	Cov(X,Y)=E(XY)-E(X)E(Y)
# 推导:
Cov(X,Y)=E( (X-EX)*(Y-EY) )=E( XY-XEY-YEX+EXEY )
=E(XY)-EXEY-EYEX+EXEY
=E(XY)-EXEY

我们注意到，最后的结果看着很熟悉！
	就是前面提到的性质：如果X和Y相互独立，则 E(XY)=E(X)E(Y)
	而 Cov(X,Y)=E(XY)-EXEY
	独立，则协方差等于0。
		但是独立，不见得协方差等于0.
		所以协方差=0这个条件，比独立弱。
		我们就把协方差为0，定义为不相关。
			其实是相关系数为0，其实相关系数的分子是协方差。
#
不相关，是说线性不相关，并不是独立。


(2) 协方差的意义
协方差曲线什么样的？有没有上届？

1)协方差是一个两头低、中间高的曲线。
2) 
Var(X)=sigma1^2;
Var(Y)=sigma2^2;
则 |Cov(X,Y)|<=sigma1*sigma2; ---> 下面给出证明。
当且仅当X和Y之间有线性关系时，等号成立。

PCA 主成分分析
ICA 独立成分分析

证明:
(Cov(X,Y))^2 = (E( (X-EX)(Y-EY) ))^2 # 协方差定义
<= (E( (X-EX)^2 * (Y-EY)^2 )) # 方差的性质 E(X^2) >= (EX)^2
<= (E( (X-EX)^2 * (Y-EY)^2 )) #期望的性质? E(XY)-EX*EY 符号不定.
=Var(X)*Var(Y) #方差的定义。
注意: 因为第三行不成立，所以这个证明是错的。


证明:
使用“柯西施瓦茨方法” 就是构造一个抛物线，自变量是X和Y的协方差。设定判别式小于等于0 就可以求出取值范围 


i) 取任意实数t，构造随机变量Z
Z=(X-EX)*t + (Y-EY)
两边同时取平方再取期望 E(Z^2)=sigma1*t^2 + 2Cov(X,Y)*t + sigma2
因为 E(Z^2) >=0
所以 sigma1*t^2 + 2Cov(X,Y)*t + sigma2 >=0
这是一个关于t的一元二次函数， 非负，则必然没有实根或只有一个根，也就是判别式小于等于0.

回顾高中数学，求根判定法 f(x)=a*x^2+b*x+c
delta=b^2-4*a*c<=0时无解或者有1个解。

所以 delta=4*cov^2(X,Y) - 4*sigma1*sigma2<=0
也就是 |Cov(X,Y)|<=sqrt(sigma1*sigma2);



ii) 探讨一下什么时候等号成立呢？
如果等号成立，原方程只有一个实根，也即只有一个t，使得等式成立
sigma1*t^2 + 2Cov(X,Y)*t + sigma2 =E(Z^2)=0; Z=0
(X-EX)*t + (Y-EY)=0
这个能否推出 Y=t*X+b ?
应该可以，怎么证明呢？

(x1-EX)*t + (y1-EY)=0
(x2-EX)*t + (y2-EY)=0
...
(xn-EX)*t + (yn-EY)=0
可见，如果X和Y确定，EX和EY确定。
y1=-t*(x1-EX)+EY=-t*x1+(t*EX+EY)
所以 Y=a*X+b 是成立的。

就是 X和Y 成线性时，协方差最大。




(3) Person 相关系数 
我们接着定义X和Y的相关系数为 Row(x,y) = Cov(X,Y) / sqrt(sigma1*sigma2)

怎么证明 |Row(X,Y)|<=1呢？

接着看到另一个推导: https://www.zhihu.com/question/24598125
下面之所以又重复一遍，是因为上文证明是错的，已经订正过了。

先证明 E(X,Y)^2 <= E(X^2)*E(Y^2)

令 g(t)=E[(X+t*Y)^2]=E(X^2)+2*t*E(X,Y)+t^2*E(Y^2)>=0
delta=[2*E(X,Y)]^2 - 4*E(X^2)*E(Y^2)<=0
(E(X,Y))^2 <= E(X^2)*E(Y^2)

所以 (E(X-EX,Y-EY))^2 <= E( (X-EX)^2)*E( (Y-EY)^2)
即 [Cov(X,Y)]^2 <= var(X)*var(Y)
故 -1 <= Cov(X,Y) / sqrt(var(X)*var(Y)) <= 1


当且仅当X和Y有线性关系时，等号成立。 

容易看出，相关系数是标准尺度下的协方差。

上面关于协方差和XY相互关系的结论，完全适用于相关系数和XY的相互关系。




python实例: pearson correlation(旋转坐标系)

数据旋转公式
x1=cos(angle)*x-sin(angle)*y;
y1=cos(angle)*y+sin(angle)*x;

X-Y不同函数关系，计算相关系数。
- 一次函数 倾斜角是0时，是90度时
- 二次函数 y=x^2，相关系数就是0
- 正切





(4) 协方差矩阵 
对于n个随机向量(X1,...,Xn),任意2个元素Xi和Xj都可以得到一个协方差，
从而可以形成一个nxn矩阵。

C=
|c11 c12 ... c1n|
|c21 c22 ... c2n|
...
|cn1 cn2 ... cnn|

1) 协方差矩阵式对称阵。
	从定义知道 cij=E(Xi-EXi)(Xj-EXj)=cji
2) 协方差矩阵还是正定的。
如何证明呢？主对角线上从第一个开始的代数余子式都大于0.




(5) 联想与思考
1) 如果X和Y不相关，则E(X,Y)=EX*EY;
2) 如果X、Y独立，则 V(XY)=Var(X)Var(Y) + var(X)* (EY)^2 + var(Y)*(EX)^2
	这个有啥用？
		- 乘积的方差，比原方差的乘积更大。
		- 
3) 对称矩阵的不同特征值对应的特征向量，是否一定正交？


4) 对称阵和正交阵是否能建立联系？




(6) 思考 
1) 给定2个随机变量X和Y，如何对量这2个随机变量的距离/相似度？
下面的讨论看，叫相似度更好。

A: 其实，协方差Cov(X,Y)就可以作为X和Y的距离。只不过不能保证是最好的。
不相关，则 协方差=0；
y=x^2时，cov(x,y)=0, 很好的关系，协方差竟然是0! 说明不是最好的。

协方差，和 余弦相似度 有关?



## 引入相互熵
ln(p(x)*p(y)/p(x,y)) 关于p(x,y) 求期望，也就是 
f(x,y)=求和(对所有的x,y,  p(x,y) * ln(p(x)*p(y)/p(x,y)) ) 定义为x,y的距离。
- 好处是:
	如果x,y互相独立，则p(x,y)=p(x)p(y)，则 ln(p(x)*p(y)/p(x,y))=ln(1)=0
	也就是x,y独立，则相互熵f(x,y)为0.
#






========================================
|-- 切比雪夫不等式、大数定理、中心极限定理
----------------------------------------

2) 设随机变量X的期望为mu,方差为sigma^2, 对任意正数i，
试估计概率 P{|X-mu|<i}的下限。
即: 随机变量的变化值 落在期望值附近的概率。

以连续型随机变量为例
p{|x-mu|>=i}=积分( |x-mu|>=i, f(x)dx )  #已知|x-mu|>=i，所以 1<=|x-mu|/i
<= 积分( |x-mu|>=i, (|x-mu|/i)^2 * f(x)dx )
=1/i^2 * 积分(|x-mu|>=i, (x-mu)^2*f(x)dx ) #放大积分定义域到R
<=1/i^2 * 积分(-无穷大,+无穷大, (x-mu)^2*f(x)dx )
=1/i^2 * sigma^2

所以 
p{|X-mu|<i}=1-p{|x-mu|>=i} >= 1-(sigma/i)^2;




1. 切比雪夫不等式
设随机变量X的期望为mu，方差为sigma^2，对于任意正数i，有
p{|X-mu|>=i} <= sigma^2/i^2

(2) 说明:
p{|X-mu|<i} > 1-sigma^2/i^2
X的方差越小，事件{|X-mu|<i}发生的概率越大。
即：X取的值基本上集中在期望mu附近。
- 该不等式进一步说明了方差的含义。
- 该不等式可以证明大数定理。




2. 大数定理
设随机变量X1,X2,...,Xn,...互相独立，并且具有相同的期望mu和方差sigma^2.
做前n个随机变量的平均Yn=1/n*求和(i=1,n, Xi)，则对于任意整数 ips，有 
lim(n->无穷大, P{|Yn-mu|< ips}) = 1

简单总结: 均值依概率收敛于期望。

如何证明: ? //todo



(2) 重要推论 
一次试验中事件A发生的概率为p；
重复n次独立试验中，事件A发生了nA次，则p,n,nA的关系满足：
对任意整数 ips,
lim(n->无穷大, |nA/n - p|<ips) = 1

这就是 伯努利大数定理。

事件A的频率以概率收敛于事件A的概率p.




3. 中心极限定理 Central limit theorem

独立同分布 iid: 

设随机变量X1,X2,...,Xn...互相独立，服从同一分布，
并且具有相同的期望mu和方差sigma^2，则随机变量
Yn= ( 求和(i=1,n,Xi) - n*mu )/(sqrt(n)*sigma)
的分布收敛到标准正态分布N(0,1)。

容易得到: 求和(i=1,n,Xi)收敛到正态分布N(n*mu, n*sigma^2)


(2) 可以py实验验证。
对于很多均匀分布，仅仅逐位加和后，得到正态分布。

import numpy as np
import matplotlib.pyplot as plt

def test3():
    u=np.random.uniform(0.0, 1.0, 10000)
    plt.hist(u, 80, facecolor='g', alpha=0.75)
    plt.grid(True)
    plt.show()
    
    times=10000
    for time in range(times):
        u+=np.random.uniform(0.0, 1.0, 10000)
    print(len(u))
    u /= times
    print(len(u))
    plt.hist(u, 80, facecolor='g', alpha=0.75)
    plt.grid(True)
    plt.show()
test3()




4. 例题 
用户年龄，均值25，标准差2，试估计21-29的概率至少是多少？
解: 没有说分布，只好使用车比雪夫不等式。
p{|X-mu|<i} > 1-sigma^2/i^2

P{21<X<29}=P{|X-25|<4}>=1-2^2/4^2=75%







========================================
R语言常用函数
----------------------------------------
排列数
A=function(N,K){ choose(N,K) * factorial(K) }

组合数 choose(N,K)

阶乘 factorial(K)







========================================
>>>>>>>>>   学习进度条
----------------------------------------

https://www.bilibili.com/video/BV1Tb411H7uC?p=2
9.11
24.47 16/98
29  17/98
37 (2020.8.3)
55 31/98(2020.8.6)
70 (2020.8.15)
95 (2020.8.18)
110 (2020.8.18)
116 (2020.8.22)
126 (2020.8.27)
135 (2020.8.30)
150 (2020.9.2)





========================================
----------------------------------------


========================================
----------------------------------------


