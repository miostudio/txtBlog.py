机器学习教程【强烈推荐】

小象学院
https://www.bilibili.com/video/BV1Tb411H7uC



========================================
1.机器学习概论(142min)
----------------------------------------
1. 定义: 
机器学习=f(任务T, 经验E, 时间T, 性能P)


问题: 如何设计自动驾驶系统?
边做，边反馈，逐渐修正，越来越正确。

如何从无知，到掌握知识。


有监督的学习: 小孩学月亮
无监督的学习: 阅兵
增强学习: 走路、踢球


加部分标签，其余knn自动加标签：版监督模型。





2. 机器学习的内涵和外延
	数据清晰/特征选择
	确定算法模型/参数优化
	结果预测
#

推导公式、写实现代码，有助于理解调参的意义。
实际工作中，使用现有库，掉包侠。






3. 实例: 线性回归预测房价。
feature: 
	type
	room
	squre
	public transport
#
price:
#
损失函数 Lost(theta)= 累加 (f(t)-realValue)^2
也叫目标函数。

随着theta的变动，损失函数越来越小，则是最优的。


难点: 怎么建模? 目标函数怎么建立?

theta是可以根据模型自动优化的。
还有些超参，只能人工设定，根据经验。


这个预测模型的过程: 
(1)tranining:
Input
	text docs
	images
	sounds
	transactions
i)Feature vectors
ii)Machine learning algorithm
iii)Model


(2)New input
	text docs
	images
	sounds
	transactions
i)Feature vectors
ii)use model;
iii) Expected Label



4. 机器学习的一般流程
数据收集
数据清洗
特征工程
数据建模



5. 模型选择的依据？
先大致了解各种模型的方法
最好会推导，然后具体问题具体分析。


因马尔科夫模型HMM，用于发现新词: 





========================================
|-- 机器学习实例、落地
----------------------------------------

6. 落地语言: Python 
分析
画图: 

# 实例1： 在log曲线上连接两点
import math
import matplotlib.pyplot as plt
def myDraw():
    x=[float(i)/100.0 for i in range(1,300)]
    y=[math.log(i) for i in x]
    plt.plot(x,y, 'r-', linewidth=3, label='log Curve')
    #
    a=[x[20], x[175]]
    b=[y[20], y[175]]
    plt.plot(a,b,'g-', linewidth=2)
    plt.plot(a,b,'b-', markersize=15, alpha=0.75)
    #
    plt.legend(loc="upper left")
    plt.grid(True)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.show()
myDraw()


## 实例2: 做模型验证：验证中心极限定理
import numpy as np
import matplotlib.pyplot as plt
def validateModel():
    #均匀分布
    u=np.random.uniform(0.0, 1.0, 10000)
    plt.hist(u, 80, facecolor='g', alpha=0.75)
    plt.grid(True)
    plt.show()
    #
    # 1万个均匀分布累加/n，很接近正态分布(又叫 高斯分布)
    times=10000
    for time in range(times):
        u += np.random.uniform(0.0, 1.0, 10000)
    print(len(u))
    u /= times
    print(len(u))
    #
    plt.hist(u, 80, facecolor='g', alpha=0.75)
    plt.grid(True)
    plt.show()
#
validateModel()


实例3: 线性回归、rate、Loss 
无代码


实例4: EM code
有1000个身高数据，假定男性身高符合 N(mu1, sigma1^2), 女性身高符合 N(mu2, sigma2^2)，
x个男性和y个女性混合到一起，构成一个高斯混合模型。
可以使用EM模型推测：男女的分布参数mu1,mu2, sigma1,sigma2都是什么，男女比例多少。


实例5: EM 算法: 无监督分类鸢尾花数据
横轴身高、纵轴体重，也可以使用EM算法推测构成，及参数。


等高线图，画的其实是似然函数的折线图。



实例5: 图形前景色、背景色区分



实例6: 图像的卷积。
卷积神经网络 CNN
不同的算子得到不同的结果，然后就可以上分类器了：随机森林，logistic回归。


实例7: 去均值ICA分离
源信号1: 独立成分1
源信号2: 独立成分2
混合信号1: 混合信号2

带噪声的信号分离。


实例8: 高斯核函数的影响
SVM的支撑向量、过渡带
用高斯分布，过渡曲线，则分类效果更精准。


实例9: crawler 爬取数据
HMM分词: MLE，很多做分词的算法

LDA 主题模型分布、词分布

舆情:
	获取QQ群聊天记录: 文本格式
	整理成 QQ号/时间/留言 的规则形式
		- 正则表达式
		- 清洗特定词：表情、@xx
		- 使用停止词库
		- 获得csv表格数据
	合并相同QQ号的留言
		长文档利于计算每人感兴趣话题
	LDA 模型计算主题
		调参与可视化
	计算每个QQ号及众人感兴趣话题
#

实例10: 石油例行检查结果处理
通过主题模型方案，分析例行检查结果中最突出的问题是什么？
文本共4700个
单个文档+数字



实例11：其他内容
- 最大熵模型: 自然语言处理解决标记问题
- 聚类: k-means / k-mediods / 密度聚类 /谱聚类
- 降维: PCA/ SVD/ ICA
- SVM: 与核技术相结合
- 主题模型pLSA/LDA: 与聚类、标签传递算法相结合
- 条件随机场: 无向图模型、链式条件随机场解决标记问题
- 变分推导Variation Inference: 与EM、贝叶斯相结合，参数、隐变量的学习
- 深度学习: 大规模人工神经网络

希望大家在课程结束时，都能搞明白这些。




========================================
|-- 本课程参考文献 (非常重要)
----------------------------------------
- Christopher M Bishop: Pattern Recognition and Machine Learning, Spinger 2006;
	世界上最好的PRML
	缺点: 1太厚了; 2 贝叶斯观点解释问题太多了，公式太多，以至于描述的很复杂。
- Kevin: Machine Learning: A Probabilistic Perspective, MIT 2012
	和PRML互补，补充前者没有的细节
	
- 李航，统计学习方法，清华大学出版社，2012
- Stephen Boyd: Convex Optimization: Cambridge University 2004
	图优化
- Thomas M, Elements of Information Theory, 2006
	信息论
- 各章节特定的经典论文，如: ...



========================================
|-- 数学背景: 
----------------------------------------

1. 回忆级数: 求S的值
S= 累加(n=0,无穷大, 1/n!) =1/0! + 1/1! + 1/2! + 1/3! +...+ 1/n! +... 

## my code: 直接算 
def base0(n):
    arr=[1]
    for i in range(1,n):
        arr.append(arr[-1]*i)
    return(arr)
#
#
num=20
for j in range(1,num):
    arr2=base0(j)
    s=0
    for i in range(len(arr2)):
       s+=1/arr2[i] 
    print(j, s) #看来结果是 自然对数的底 e
#



(2) 对数函数的上升速度
import math
import matplotlib.pyplot as plt
import numpy as np

def test1():
    x=np.arange(0.05, 3, 0.05)
    y1=np.log(x)/math.log(1.5)
    #print(np.log(2.71828), math.log(2.71828))
    plt.plot(x,y1, linewidth=2, color='#007500',label="log1.5(x)")
    #
    plt.plot([1,1], [y1[0], y1[-1]], 'r--', linewidth=2)
    #
    y2=np.log(x)/math.log(2)
    plt.plot(x,y2, linewidth=2, color='#9f35ff',label="log2(x)")
    #
    y3=np.log(x)/math.log(3)
    plt.plot(x,y3, linewidth=2, color='#f75000',label="log3(x)")
    #
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()
test1()
# 底>1时单调递增，越大上升越慢。
# 函数f(x)=log a(x) 一定过(1,0)点。

那么在(1,0)点，也就是x=1时，底a为多少时切线斜率为1呢？
i) my:
log a(x)'=(ln(x)/ln(a))'=1/x /ln(a)=1，带入位置x=1， ln(a)=1, a=e;

ii) 视频: 这里用了大量微积分推导
自然对数 lim(x->无穷大时, (1+1/x)^x )=e;

先是用an=(1+1/n)^n的二项展开，证明{an}数组有上界，该函数还单调递增，则必有极限，记为e.


结论:
	- 导数就是曲线的斜率，是曲线变化的快慢程度的反应。
	- 二阶导数是斜率的变化快慢的反应，表征曲线凹凸性。
		- 二阶导数连续的曲线，往往称之为"光滑"的
		- 高中物理: 加速度的方向总是指向轨迹曲线凹的一侧。
	- 根据 lim(x->无穷大时, (1+1/x)^x )=e 可以得到函数 f(x)=ln(x)的导数，
		进一步根据换底公式、反函数求导等，得到其他初等函数的导数。
#





2. 常用函数的导数






https://www.bilibili.com/video/BV1Tb411H7uC?from=search&seid=14407332718383043384
69 min









========================================
----------------------------------------


========================================
----------------------------------------








========================================
----------------------------------------


========================================
----------------------------------------








========================================
----------------------------------------


========================================
----------------------------------------






========================================
----------------------------------------


========================================
----------------------------------------






========================================
----------------------------------------


========================================
----------------------------------------






========================================
----------------------------------------


========================================
----------------------------------------








========================================
----------------------------------------

